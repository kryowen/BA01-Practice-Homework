{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"6lsoVITwxIqo"},"outputs":[],"source":["# import os\n","from glob import glob\n","\n","import librosa\n","from librosa.filters import mel as librosa_mel_fn\n","\n","import torch\n","from torch import nn\n","import torch.nn.functional as F\n","\n","import torchaudio\n","from torch.utils.data import Dataset, DataLoader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S_gH13kdxRbB"},"outputs":[],"source":["# GPU 사용 여부 확인\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"zvfpnTTyxUaM"},"outputs":[],"source":["import random\n","\n","# 난수 발생을 위한 seed를 모두 0으로 설정\n","torch.manual_seed(0)\n","if torch.cuda.is_available():\n","    torch.cuda.manual_seed(0)\n","\n","random.seed(0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Ws5Aw2HxdNI"},"outputs":[],"source":["from google.colab import drive\n","\n","# 구글 드라이브 마운트\n","drive.mount('/content/gdrive', force_remount=True)\n","filepath = '/content/gdrive/My Drive' + '/Colab Notebooks/speech_emotion_recognition/'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"crstOCTUySQT"},"outputs":[],"source":["class SpeechDataset(Dataset):\n","    def __init__(self, n_fft=1024, n_mels=80, sr=22050, hop_size=256, \\\n","                 win_size=1024, fmin=0, fmax=8000, max_sec=6.0, center=False):\n","        # wav 파일 가져오기\n","        # os.getcwd() : 현재의 경로\n","        # wav 디렉토리 안에 Actor별로 디렉토리에 있는 모든 wav 파일을 가져옴\n","        # Colab에서 os를 활용하기 보다는 직접 자신의 경로를 맞게 다시 설정해주자!\n","        # 예시 : sorted(glob(\"/content/gdrive/your_path/wav/*/*.wav\"))\n","        self.wav_path = sorted(glob(filepath + \"*/*.wav\"))\n","        self.wav_len = len(self.wav_path)\n","        self.n_fft = n_fft # FFT 사이즈\n","        self.n_mels = n_mels # Mel-frequency의 개수\n","        self.sr = sr # 샘플링 레이트\n","        self.hop_size = hop_size # hop length\n","        self.win_size = win_size # window length\n","        self.fmin = fmin # 최저 주파수\n","        self.fmax = fmax # 최고 주파수\n","        self.center = center # True면 padding을 좀 주고, False면 padding이 없고\n","        self.max_length = int(sr * max_sec) # 서로 다른 wav의 길이를 모두 동일하게 맞추기\n","\n","        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","        self.mel_basis = {} # Mel Filter Bank 만들어두기\n","        self.hann_window = {} # hanning window 만들어두기\n","\n","        self.mel = librosa_mel_fn(sr=self.sr, n_fft=self.n_fft, \\\n","                            n_mels=self.n_mels, fmin=self.fmin, fmax=self.fmax)\n","        self.mel_basis[self.device] = torch.from_numpy(self.mel).float().to(self.device)\n","        self.hann_window[self.device] = torch.hann_window(self.n_fft).to(self.device)\n","\n","\n","    def __len__(self): # len()에 대응하는 스페셜 메소드\n","        return self.wav_len\n","\n","    def __getitem__(self, idx): # 인덱싱을 지원하는 Operator 개념?\n","        emotion = self.get_emotion(self.wav_path[idx])  # emotion label 가져오기\n","\n","        waveform = self.load_audio(self.wav_path[idx]) # waveform 획득\n","        spec = self.mel_spectrogram(waveform).to(self.device) # wav → mel-spectrogram\n","\n","        return spec, emotion\n","\n","    def load_audio(self, file_path):\n","        waveform, sr = torchaudio.load(file_path) # wav 파일 load\n","        waveform = waveform.to(self.device) # wav 파일 로드\n","\n","        # 2채널 이상이면, mono 채널로 만들기\n","        if waveform.size(0) != 1:\n","            waveform = waveform.mean(dim=0).unsqueeze(0)\n","\n","        # 샘플링 레이트 안맞으면 re-sampling\n","        if sr != self.sr:\n","            resampler = torchaudio.transforms.Resample(sr, self.sr).to(self.device)\n","            waveform = resampler(waveform)\n","\n","        # wav normalization\n","        waveform = waveform / torch.max(torch.abs(waveform))\n","\n","        # zero-padding 얼마나 할지\n","        pad = int(self.max_length - waveform.size(1) - self.hop_size) // 2\n","\n","        # wav 양쪽에 zero-padding\n","        waveform = torch.nn.functional.pad(waveform, (pad, pad), mode='constant')\n","\n","        return waveform\n","\n","    def mel_spectrogram(self, y): # wavform -> mel-spectrogram 변환\n","        # short-time Fourier transform (STFT)\n","        spec = torch.stft(y, self.n_fft, hop_length=self.hop_size, win_length=self.win_size,  \\\n","                          window=self.hann_window[self.device], center=self.center, \\\n","                          pad_mode='reflect', normalized=False, onesided=True, return_complex=True)\n","        spec = torch.view_as_real(spec)  # Complex Tensor를 Real Tensor로 변환\n","        spec = torch.sqrt(spec.pow(2).sum(-1) + (1e-9))\n","        # Complex -> Spectrogram(Magnigtude)\n","        spec = torch.matmul(self.mel_basis[self.device], spec)\n","        # Spectrogram(Magnitude) -> Mel-Spectrogram\n","        spec = torch.log(torch.clamp(spec, min=1e-5))  # Power to Decibel\n","\n","        return spec\n","\n","    # 파일 이름에서 emotion 정보 가져오기\n","    def get_emotion(self, file_path):\n","        # wav 파일명은 아래와 같은 규칙을 가지고 있음\n","        # 02-01-06-01-02-01-12.wav\n","        # 세번째 항목(06)이 emotion label -> 해당 파일은 \"fearful\"에 해당함\n","        # (01=neutral, 02=calm, 03=happy, 04=sad, 05=angry, 06=fearful, 07=disgust, 08=surprised)\n","        # 데이터에 대해 더 궁금한건 \"Kaggle - Speech Emotion Recognition\"을 검색해보자!\n","        part = file_path.split(\"Actor\")[1]\n","        emotion = int(part.split(\"-\")[2])\n","        return torch.LongTensor([emotion - 1]).squeeze().to(self.device)"]},{"cell_type":"code","source":[],"metadata":{"id":"klGgVPCi43VY"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uJhfc7bxyVq0"},"outputs":[],"source":["speech_dataset = SpeechDataset()\n","\n","# 데이터셋 인덱스 생성\n","indices = list(range(len(speech_dataset)))\n","random.shuffle(indices)\n","\n","# 데이터셋 분할 인덱스 계산\n","train_ratio = 0.7\n","validation_ratio = 1 - train_ratio\n","train_split_index = int(train_ratio * len(speech_dataset))\n","\n","# train 데이터셋과 validation 데이터셋 생성\n","train_dataset = torch.utils.data.Subset(speech_dataset, indices[:train_split_index])\n","val_dataset = torch.utils.data.Subset(speech_dataset, indices[train_split_index:])\n","\n","train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gWfydki_yifS"},"outputs":[],"source":["class CNN(nn.Module):\n","    def __init__(self, num_classes=8):\n","        super().__init__()\n","\n","        self.cnn = nn.Sequential (\n","            nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),\n","            # [b, 1, 80, 512] -> [b, 16, 80, 512]\n","            nn.ReLU(True),\n","            nn.MaxPool2d(2, stride=2),\n","            # [b, 16, 80, 512] -> [b, 16, 40, 256]\n","            nn.Dropout(0.1),\n","\n","            nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1),\n","            # [b, 16, 40, 256] -> [b, 16, 40, 256]\n","            nn.ReLU(True),\n","            nn.MaxPool2d(2, stride=2),\n","             # [b, 16, 40, 256] -> [b, 16, 20, 128]\n","            nn.Dropout(0.1),\n","\n","            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n","             # [b, 16, 20, 128] -> [b, 32, 20, 128]\n","            nn.ReLU(True),\n","            nn.MaxPool2d(2, stride=2),\n","            # [b, 32, 20, 128] -> [b, 32, 10, 64]\n","            nn.Dropout(0.1),\n","\n","            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1),\n","             # [b, 32, 10, 64] -> [b, 32, 10, 64]\n","            nn.ReLU(True),\n","            nn.MaxPool2d(2, stride=2),\n","             # [b, 32, 10, 64] -> [b, 32, 5, 32]\n","            nn.Dropout(0.1),\n","        )\n","\n","        self.fc1 = nn.Linear(32 * 5 * 32, 256)\n","        self.fc2 = nn.Linear(256, num_classes)\n","\n","    def forward(self, x):\n","        x = self.cnn(x)\n","        x = torch.flatten(x, start_dim=1)\n","        x = F.relu(self.fc1(x))\n","        x = self.fc2(x)\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hyv_A527xvur"},"outputs":[],"source":["\n","\n","from torch.optim import Adam\n","\n","model = CNN().to(device)\n","criterion = nn.CrossEntropyLoss().to(device)\n","opti = Adam(model.parameters(), lr=1e-3, weight_decay=1e-3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NCf1BTRcxx-u"},"outputs":[],"source":["from tqdm.notebook import tqdm\n","\n","def train(model, dataloader, criterion, data_len, opti):\n","    correct = 0\n","\n","    model.train()\n","    for data, target in tqdm(dataloader):\n","        data = data.to(device)\n","        target = target.to(device)\n","\n","        output = model(data)\n","        loss = criterion(output, target)\n","\n","        opti.zero_grad()\n","        loss.backward()\n","        opti.step()\n","\n","        pred = output.max(1, keepdim=True)[1]\n","        correct += pred.eq(target.view_as(pred)).sum().item()\n","\n","    acc = 100. * correct / data_len\n","    return acc"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AXRPYEfgyCRc"},"outputs":[],"source":["def evaluate(model, dataloader, criterion, data_len):\n","    correct = 0\n","\n","    model.eval()\n","    for data, target in tqdm(dataloader):\n","        data = data.to(device)\n","        target = target.to(device)\n","\n","        output = model(data)\n","        loss = criterion(output, target)\n","\n","        pred = output.max(1, keepdim=True)[1]\n","        correct += pred.eq(target.view_as(pred)).sum().item()\n","\n","    acc = 100. * correct / data_len\n","    return acc"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZVoxbGakyLAq"},"outputs":[],"source":["# .wav를 .npz로 변경해서 학습시간을 줄이는 것이 유리해보인다.\n","# wav → MFCC로 변경하는 건가?\n","epoch = 20\n","\n","for i in range(epoch):\n","    train_acc = train(model, train_dataloader, criterion, len(train_dataloader.dataset), opti)\n","    val_acc = evaluate(model, val_dataloader, criterion, len(val_dataloader.dataset))\n","\n","    print(f\"[Epoch: {i:2d}], [Train Acc: {train_acc:3.4f}], [Val Acc: {val_acc:3.4f}]\" + '\\n')"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"authorship_tag":"ABX9TyM3RHQwtbLaP+hu1fL3fO7k"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}